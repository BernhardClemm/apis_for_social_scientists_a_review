library(pagedown)
library(tidyverse)
library(googleLanguageR)
library(tidytext)
library(ggwordcloud)
# Authentication (through your service account's JSON key file)
gl_auth("C:/Users/Paul/Google Drive/2-Teaching/2021 Computational Social Science/keys/css-seminar-2021-a1e75382ae2c.json")
data_syntax <- gl_nlp(data_tweets$translatedText, nlp_type = "analyzeSyntax")
load("C:/Users/Paul/Google Drive/2-Teaching/2021 Computational Social Science/2021_computational_social_science/data/data_tweets.RData")
library(tidyverse)
library(googleLanguageR)
library(tidytext)
library(ggwordcloud)
# Authentication (through your service account's JSON key file)
gl_auth("C:/Users/Paul/Google Drive/2-Teaching/2021 Computational Social Science/keys/css-seminar-2021-a1e75382ae2c.json")
# Load data
load("C:/Users/Paul/Google Drive/2-Teaching/2021 Computational Social Science/2021_computational_social_science/data/data_tweets.RData")
# Call the API via the function 'gl_nlp()' and specify your quantity of interest (here: analyzeSyntax)
data_syntax <- gl_nlp(data_tweets$translatedText, nlp_type = "analyzeSyntax")
data_tweets$syntax_tokens <- data_syntax[["tokens"]]
data_syntax
gl_translate(data_tweets$text, target ="en")
gl_translate(data_tweets$text, target ="en")gl_translate(data_tweets$text, target ="en")$translatedText
gl_translate(data_tweets$text, target ="en")$translatedText
bind_cols(data_tweets,
text_en = gl_translate(data_tweets$text, target ="en")$translatedText)
data_tweets <-
bind_cols(data_tweets,
text_en = gl_translate(data_tweets$text,
target ="en")$translatedText)
data_syntax <- gl_nlp(data_tweets$test_en, nlp_type = "analyzeSyntax")
names(data_tweets)
setwd("C:/Users/camil/Desktop/apis_for_social_scientists_a_review")
getwd()
# Load packages
library(tidyverse)
library(googleLanguageR)
library(tidytext)
library(ggwordcloud)
library(quanteda.corpora)
# Authentication (through your service account's JSON key file)
gl_auth("./keys/paul-css-seminar-2021-a1e75382ae2c.json")
# Load data
guardian_corpus <- quanteda.corpora::download("data_corpus_guardian")
View(guardian_corpus)
# Extract text only rom the corpus
texts <- guardian_corpus[["documents"]][["texts"]]
# For demonstration purposes, subset the text data to 20 observations only
texts <- texts[1:20]
# Turn text into a data frame and add an identifier
df <- as.data.frame(texts)
df <- tibble::rowid_to_column(df, "ID")
# Load corpus
guardian_corpus <- quanteda.corpora::download("data_corpus_guardian")
# Keep text only from the corpus
text <- guardian_corpus[["documents"]][["texts"]]
# For demonstration purposes, subset the text data to 20 observations only
text <- text[1:20]
# Turn text into a data frame and add an identifier
df <- as.data.frame(text)
df <- tibble::rowid_to_column(df, "ID")
data_syntax <- gl_nlp(df$texts, nlp_type = "analyzeSyntax")
View(data_syntax)
# Authentication (through your service account's JSON key file)
gl_auth("./keys/paul-css-seminar-2021-a1e75382ae2c.json")
data_syntax <- gl_nlp(df$texts, nlp_type = "analyzeSyntax")
View(data_syntax)
data_syntax <- gl_nlp(df$text, nlp_type = "analyzeSyntax")
View(data_syntax)
glimpse(data_syntax)
data_syntax
str(data_syntax)
head(data_syntax[["tokens"]][[1]][,1:5])
syntax_analysis <- gl_nlp(df$text, nlp_type = "analyzeSyntax")
View(syntax_analysis)
head(syntax_analysis[["tokens"]][[1]][,1:3])
head(syntax_analysis[["tokens"]][[1:3]][,1:3])
head(syntax_analysis[["tokens"]][[1]][,1,3])
head(syntax_analysis[["tokens"]][[1]][,1:3])
head(syntax_analysis[["tokens"]][[1]][,1:3])
# Add tokens from syntax analysis to original dataframe
df$tokens <- syntax_analysis[["tokens"]]
# Keep nouns only
df <- df %>%
mutate(tokens_nouns = map(syntax_tokens,
~ filter(., tag == "NOUN")))
# Keep nouns only
df <- df %>%
mutate(tokens_nouns = map(tokens,
~ filter(., tag == "NOUN")))
View(df)
# Download and store corpus
guardian_corpus <- quanteda.corpora::download("data_corpus_guardian")
# Keep text only from the corpus
text <- guardian_corpus[["documents"]][["texts"]]
# For demonstration purposes, subset the text data to 20 observations only
text <- text[1:20]
# Turn text into a data frame and add an identifier
df <- as.data.frame(text)
df <- tibble::rowid_to_column(df, "ID")
syntax_analysis <- gl_nlp(df$text, nlp_type = "analyzeSyntax")
head(syntax_analysis[["tokens"]][[1]][,1:3])
# Add tokens from syntax analysis to original dataframe
df$tokens <- syntax_analysis[["tokens"]]
# Keep nouns only
df <- df %>%
mutate(tokens_nouns = map(tokens,
~ filter(., tag == "NOUN")))
View(df)
# Download and store corpus
guardian_corpus <- quanteda.corpora::download("data_corpus_guardian")
# Keep text only from the corpus
text <- guardian_corpus[["documents"]][["texts"]]
# For demonstration purposes, subset the text data to 20 observations only
text <- text[1:20]
# Turn text into a data frame and add an identifier
df <- as.data.frame(text)
df <- tibble::rowid_to_column(df, "ID")
View(df)
View(df)
# Add tokens from syntax analysis to original dataframe
df$tokens <- syntax_analysis[["tokens"]]
View(df)
df$tokens
# Keep nouns only
df <- df %>%
mutate(nouns = map(tokens,
~ filter(., tag == "NOUN")))
df$nouns
# Create the data for the plot
data_plot <- df %>%
# only keep content variable
mutate(tokens_nouns = map(tokens_nouns,
~ select(., content))) %>%
# Write tokens in all rows into a single string
unnest(tokens_nouns) %>% # unnest tokens
# Unnest tokens
unnest_tokens(output = word, input = content) %>% # generate a wordcloud
anti_join(stop_words) %>%
dplyr::count(word) %>%
filter(n > 10) #only plot words that appear more than 10 times
# Create the data for the plot
data_plot <- df %>%
# only keep content variable
mutate(nouns = map(nouns,
~ select(., content))) %>%
# Write tokens in all rows into a single string
unnest(nouns) %>% # unnest tokens
# Unnest tokens
unnest_tokens(output = word, input = content) %>% # generate a wordcloud
anti_join(stop_words) %>%
dplyr::count(word) %>%
filter(n > 10) #only plot words that appear more than 10 times
# Visualize in a word cloud
data_plot %>%
ggplot(aes(label = word,
size = n)) +
geom_text_wordcloud() +
scale_size_area(max_size = 10) +
theme_minimal()
library(tidyverse)
library(googleLanguageR)
library(tidytext)
# Authentication (through your service account's JSON key file)
gl_auth("./keys/paul-css-seminar-2021-a1e75382ae2c.json")
# Load package
library(quanteda.corpora)
# Download and store corpus
guardian_corpus <- quanteda.corpora::download("data_corpus_guardian")
# Keep text only from the corpus
text <- guardian_corpus[["documents"]][["texts"]]
# For demonstration purposes, subset the text data to 20 observations only
text <- text[1:20]
# Turn text into a data frame and add an identifier
df <- as.data.frame(text)
df <- tibble::rowid_to_column(df, "ID")
syntax_analysis <- gl_nlp(df$text, nlp_type = "analyzeSyntax")
save(syntax_analysis, file="CH_2_syntax_analysis.RData")
load("Chapter_2_df.RData")
library(tidyverse)
library(googleLanguageR)
library(tidytext)
# Authentication (through your service account's JSON key file)
gl_auth("./keys/trustme-312210-41f50915e801.json")
getwd()
# Authentication (through your service account's JSON key file)
gl_auth("./keys/trustme-312210-41f50915e801.json")
gl_auth("./keys/trustme-312210-41f50915e801.json")
# Load package
library(quanteda.corpora)
# Download and store corpus
guardian_corpus <- quanteda.corpora::download("data_corpus_guardian")
# Keep text only from the corpus
text <- guardian_corpus[["documents"]][["texts"]]
# For demonstration purposes, subset the text data to 20 observations only
text <- text[1:20]
# Turn text into a data frame and add an identifier
df <- as.data.frame(text)
df <- tibble::rowid_to_column(df, "ID")
syntax_analysis <- gl_nlp(df$text, nlp_type = "analyzeSyntax")
View(syntax_analysis)
save(syntax_analysis, file="./Chapter_2/Chapter_2_df.RData")
save(syntax_analysis, file="./Chapter_2/Chapter_2_df.RData")
load("./Chapter_2/Chapter_2_df.RData")
load("C:/Users/camil/Desktop/apis_for_social_scientists_a_review/Chapter_2_Google_NLP_API_cache/html/unnamed-chunk-12_1c60a282b667edb2af6da6e55eb013e2.RData")
load("C:/Users/camil/Desktop/apis_for_social_scientists_a_review/Chapter_2_Google_NLP_API_cache/html/unnamed-chunk-13_56aad346bfd18c7d9a86d5a69fb81c31.RData")
load("C:/Users/camil/Desktop/apis_for_social_scientists_a_review/Chapter_2_Google_NLP_API_cache/html/unnamed-chunk-13_56aad346bfd18c7d9a86d5a69fb81c31.RData")
head(syntax_analysis[["tokens"]][[1]][,1:3])
